{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINGLE LAYER PERCEPTRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"9_Iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "data=data.drop(columns=['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4            5.0           3.6            1.4           0.2  Iris-setosa\n",
       "5            5.4           3.9            1.7           0.4  Iris-setosa\n",
       "6            4.6           3.4            1.4           0.3  Iris-setosa\n",
       "7            5.0           3.4            1.5           0.2  Iris-setosa\n",
       "8            4.4           2.9            1.4           0.2  Iris-setosa\n",
       "9            4.9           3.1            1.5           0.1  Iris-setosa"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns=['SepalLengthCm', 'SepalWidthCm',\n",
    "'PetalLengthCm', 'PetalWidthCm', 'Species']\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_func(value):\n",
    "    return ((np.exp(value)-np.exp(-value))/(np.exp(value)+np.exp(-value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_train(in_data, labels, alpha):\n",
    "    X = np.array(in_data)\n",
    "    y = np.array(labels) # Added closing parenthesis here\n",
    "    weights = np.random.random(X.shape[1])\n",
    "    original = weights\n",
    "    bias = np.random.random_sample()\n",
    "    for key in range(X.shape[0]):\n",
    "        a = activation_func(np.matmul(np.transpose(weights), X[key]))\n",
    "        yn = 0\n",
    "        if a >= 0.7:\n",
    "            yn = 1\n",
    "        elif a <= (-0.7):\n",
    "            yn = -1\n",
    "        weights = weights + alpha * (yn - y[key]) * X[key]\n",
    "        print('Iteration ' + str(key) + ': ' + str(weights))\n",
    "    print('Difference: ' + str(weights - original)) \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_test(in_data,label_shape,weights):\n",
    "    X=np.array(in_data)\n",
    "    y=np.zeros(label_shape)\n",
    "    for key in range(X.shape[1]):\n",
    "        a=activation_func((weights*X[key]).sum())\n",
    "        y[key] =0\n",
    "        if a>=0.7:\n",
    "            y[key]=1\n",
    "        elif a<=(-0.7):\n",
    "            y[key]=-1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(result, labels):\n",
    "    result_numeric = np.array(result).astype(float)\n",
    "    labels_numeric = np.array(labels).astype(float)\n",
    "    difference = result_numeric - labels_numeric\n",
    "    correct_ctr = np.count_nonzero(difference == 0)\n",
    "    score = correct_ctr * 100 / len(labels)\n",
    "    print('Score =', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "divider= np.random.rand(len(data)) < 0.70\n",
    "d_train=data[divider]\n",
    "d_test=data[~divider]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing a train into data and Labels/targets\n",
    "d_train_y=d_train['Species']\n",
    "d_train_X=d_train.drop(['Species'], axis=1)\n",
    "# Dividing d_train into data and Labels/targets\n",
    "d_test_y=d_test['Species']\n",
    "d_test_X=d_test.drop([\"Species\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.01\n",
    "label_encoder = LabelEncoder()\n",
    "d_train_y_encoded = label_encoder.fit_transform(d_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: [0.97771772 1.01503233 0.18391011 0.89021221]\n",
      "Iteration 1: [1.02671772 1.04503233 0.19791011 0.89221221]\n",
      "Iteration 2: [1.07271772 1.07603233 0.21291011 0.89421221]\n",
      "Iteration 3: [1.12271772 1.11203233 0.22691011 0.89621221]\n",
      "Iteration 4: [1.17671772 1.15103233 0.24391011 0.90021221]\n",
      "Iteration 5: [1.22271772 1.18503233 0.25791011 0.90321221]\n",
      "Iteration 6: [1.27271772 1.21903233 0.27291011 0.90521221]\n",
      "Iteration 7: [1.31671772 1.24803233 0.28691011 0.90721221]\n",
      "Iteration 8: [1.37071772 1.28503233 0.30191011 0.90921221]\n",
      "Iteration 9: [1.41871772 1.31903233 0.31791011 0.91121221]\n",
      "Iteration 10: [1.46671772 1.34903233 0.33191011 0.91221221]\n",
      "Iteration 11: [1.50971772 1.37903233 0.34291011 0.91321221]\n",
      "Iteration 12: [1.56771772 1.41903233 0.35491011 0.91521221]\n",
      "Iteration 13: [1.62471772 1.46303233 0.36991011 0.91921221]\n",
      "Iteration 14: [1.67871772 1.50203233 0.38291011 0.92321221]\n",
      "Iteration 15: [1.72971772 1.53703233 0.39691011 0.92621221]\n",
      "Iteration 16: [1.78671772 1.57503233 0.41391011 0.92921221]\n",
      "Iteration 17: [1.84071772 1.60903233 0.43091011 0.93121221]\n",
      "Iteration 18: [1.88671772 1.64503233 0.44091011 0.93321221]\n",
      "Iteration 19: [1.93771772 1.67803233 0.45791011 0.93821221]\n",
      "Iteration 20: [1.98771772 1.70803233 0.47391011 0.94021221]\n",
      "Iteration 21: [2.03771772 1.74203233 0.48991011 0.94421221]\n",
      "Iteration 22: [2.08971772 1.77703233 0.50491011 0.94621221]\n",
      "Iteration 23: [2.14171772 1.81103233 0.51891011 0.94821221]\n",
      "Iteration 24: [2.18971772 1.84203233 0.53491011 0.95021221]\n",
      "Iteration 25: [2.24371772 1.87603233 0.54991011 0.95421221]\n",
      "Iteration 26: [2.29571772 1.91703233 0.56491011 0.95521221]\n",
      "Iteration 27: [2.35071772 1.95903233 0.57891011 0.95721221]\n",
      "Iteration 28: [2.39971772 1.99003233 0.59391011 0.95821221]\n",
      "Iteration 29: [2.44971772 2.02203233 0.60591011 0.96021221]\n",
      "Iteration 30: [2.50471772 2.05703233 0.61891011 0.96221221]\n",
      "Iteration 31: [2.55371772 2.08803233 0.63391011 0.96321221]\n",
      "Iteration 32: [2.60471772 2.12203233 0.64891011 0.96521221]\n",
      "Iteration 33: [2.65471772 2.15703233 0.66191011 0.96821221]\n",
      "Iteration 34: [2.69971772 2.18003233 0.67491011 0.97121221]\n",
      "Iteration 35: [2.74371772 2.21203233 0.68791011 0.97321221]\n",
      "Iteration 36: [2.79371772 2.24703233 0.70391011 0.97921221]\n",
      "Iteration 37: [2.84471772 2.28503233 0.72291011 0.98321221]\n",
      "Iteration 38: [2.89271772 2.31503233 0.73691011 0.98621221]\n",
      "Iteration 39: [2.94371772 2.35303233 0.75291011 0.98821221]\n",
      "Iteration 40: [2.98971772 2.38503233 0.76691011 0.99021221]\n",
      "Iteration 41: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 42: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 43: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 44: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 45: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 46: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 47: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 48: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 49: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 50: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 51: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 52: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 53: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 54: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 55: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 56: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 57: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 58: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 59: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 60: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 61: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 62: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 63: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 64: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 65: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 66: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 67: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 68: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 69: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 70: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 71: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 72: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 73: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 74: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 75: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 76: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 77: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 78: [3.04271772 2.42203233 0.78191011 0.99221221]\n",
      "Iteration 79: [2.98471772 2.39503233 0.73091011 0.97321221]\n",
      "Iteration 80: [2.91371772 2.36503233 0.67191011 0.95221221]\n",
      "Iteration 81: [2.85071772 2.33603233 0.61591011 0.93421221]\n",
      "Iteration 82: [2.80171772 2.31103233 0.57091011 0.91721221]\n",
      "Iteration 83: [2.73471772 2.28603233 0.51291011 0.89921221]\n",
      "Iteration 84: [2.66271772 2.25003233 0.45191011 0.87421221]\n",
      "Iteration 85: [2.59771772 2.21803233 0.40091011 0.85421221]\n",
      "Iteration 86: [2.53371772 2.19103233 0.34791011 0.83521221]\n",
      "Iteration 87: [2.47571772 2.16303233 0.29691011 0.81121221]\n",
      "Iteration 88: [2.41171772 2.13103233 0.24391011 0.78821221]\n",
      "Iteration 89: [2.33471772 2.09303233 0.17691011 0.76621221]\n",
      "Iteration 90: [2.25771772 2.06703233 0.10791011 0.74321221]\n",
      "Iteration 91: [2.19771772 2.04503233 0.05791011 0.72821221]\n",
      "Iteration 92: [2.12871772e+00 2.01303233e+00 9.10108941e-04 7.05212215e-01]\n",
      "Iteration 93: [ 2.07271772  1.98503233 -0.04808989  0.68521221]\n",
      "Iteration 94: [ 1.99571772  1.95703233 -0.11508989  0.66521221]\n",
      "Iteration 95: [ 1.92871772  1.92403233 -0.17208989  0.64421221]\n",
      "Iteration 96: [ 1.85671772  1.89203233 -0.23208989  0.62621221]\n",
      "Iteration 97: [ 1.79471772  1.86403233 -0.28008989  0.60821221]\n",
      "Iteration 98: [ 1.73071772  1.83603233 -0.33608989  0.58721221]\n",
      "Iteration 99: [ 1.65671772  1.80803233 -0.39708989  0.56821221]\n",
      "Iteration 100: [ 1.59571772  1.78203233 -0.45308989  0.55421221]\n",
      "Iteration 101: [ 1.53271772  1.74803233 -0.50908989  0.53021221]\n",
      "Iteration 102: [ 1.46871772  1.71703233 -0.56408989  0.51221221]\n",
      "Iteration 103: [ 1.39971772  1.68603233 -0.61808989  0.49121221]\n",
      "Iteration 104: [ 1.33271772  1.65503233 -0.67408989  0.46721221]\n",
      "Iteration 105: [ 1.26371772  1.62403233 -0.72508989  0.44421221]\n",
      "Iteration 106: [ 1.19571772  1.59203233 -0.78408989  0.42121221]\n",
      "Iteration 107: [ 1.12871772  1.55903233 -0.84108989  0.39621221]\n",
      "Iteration 108: [ 1.06171772  1.52903233 -0.89308989  0.37321221]\n",
      "Iteration 109: [ 0.99871772  1.50403233 -0.94308989  0.35421221]\n",
      "Iteration 110: [ 0.93971772  1.47403233 -0.99408989  0.33621221]\n",
      "Difference: [ 0.013  0.494 -1.164 -0.552]\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "weights = perceptron_train(d_train_X, d_train_y_encoded, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
